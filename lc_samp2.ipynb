{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample spatial means for ACT OCSE project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from io import StringIO, BytesIO\n",
    "import xarray as xr\n",
    "import requests\n",
    "import json\n",
    "from pyproj import Geod\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.spatial import cKDTree\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil -m cp -r \\\n",
    "  \"gs://terrakio-mass-requests/KG7zkkvurzSh23Ue70C5k7groNy2/ACT23_5km/data/ver1/*\" \\\n",
    "  /home/muye/aus-env-modis/OCSE/lc23_byCells\n",
    "\n",
    "\n",
    "gsutil -m cp -r \\\n",
    "  \"gs://terrakio-mass-requests/KG7zkkvurzSh23Ue70C5k7groNy2/ACTVeg_2023_5km/data/ver1/*\" \\\n",
    "  /home/muye/aus-env-modis/OCSE/vegCode_byCells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3. intersect cells with polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import rioxarray\n",
    "from shapely.geometry import mapping\n",
    "\n",
    "def process_cell(cell_id, polygons_gdf, landcover_nc, vegtype_nc, output_dir):\n",
    "    \"\"\"\n",
    "    Process a single cell for all districts and vegetation types.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cell_polygon : geopandas.GeoDataFrame\n",
    "        The current cell geometry\n",
    "    district_polygons : geopandas.GeoDataFrame\n",
    "        All district polygons\n",
    "    landcover_nc : str\n",
    "        Path to landcover netCDF file\n",
    "    vegtype_nc : str\n",
    "        Path to vegetation type netCDF file\n",
    "    output_dir : str\n",
    "        Directory to save the output tables\n",
    "    \"\"\"\n",
    "    # Load netCDF files\n",
    "    landcover_ds = xr.open_dataset(landcover_nc)\n",
    "    landcover_ds = landcover_ds.isel(time=0)\n",
    "    vegtype_ds = xr.open_dataset(vegtype_nc)\n",
    "    vegtype_ds = vegtype_ds.isel(time=0)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Assign CRS to both datasets\n",
    "    crs = 'EPSG:32755'\n",
    "    landcover_ds.rio.write_crs(crs, inplace=True)\n",
    "    vegtype_ds.rio.write_crs(crs, inplace=True)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each district\n",
    "    polygons_pdf = polygons_gdf.set_crs('EPSG:4326')\n",
    "    polygons_transformed = polygons_gdf.to_crs('EPSG:32755')\n",
    "\n",
    "    for idx, polygon in polygons_transformed.iterrows():\n",
    "        polygon_name = polygon['DIVISION_N']\n",
    "\n",
    "        geom = [mapping(polygon.geometry)]    \n",
    "        \n",
    "        # Mask data for current polygon with improved clipping parameters\n",
    "        landcover_masked = landcover_ds.rio.clip(geom, drop=False, all_touched=True)\n",
    "        vegtype_masked = vegtype_ds.rio.clip(geom, drop=False, all_touched=True)\n",
    "        \n",
    "        # Get unique vegetation types in the masked area\n",
    "        veg_types = np.unique(vegtype_masked['var0'].values)\n",
    "        veg_types = veg_types[~np.isnan(veg_types)]\n",
    "\n",
    "\n",
    "        # Process each vegetation type\n",
    "        for veg_type in veg_types:\n",
    "            # Create vegetation type mask\n",
    "            veg_mask = (vegtype_masked['var0'].values == veg_type)\n",
    "            \n",
    "            # Process each land cover class\n",
    "            for class_idx in range(7):\n",
    "                class_data = landcover_masked[f'var{class_idx}'].values\n",
    "                \n",
    "                # Apply vegetation type mask on land cover classes\n",
    "                masked_data = np.where(veg_mask, class_data, np.nan)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                valid_pixels = np.count_nonzero(~np.isnan(masked_data))  # after mask, non-nans are pixels of this veg type\n",
    "                mean_prob = np.nanmean(masked_data)\n",
    "                \n",
    "                # Store results\n",
    "                results.append({\n",
    "                    'polygon': polygon_name,\n",
    "                    'vegetation_type': int(veg_type),\n",
    "                    'landcover_class': class_idx,\n",
    "                    'pixel_count': valid_pixels,\n",
    "                    'mean_probability': mean_prob\n",
    "                })\n",
    "    \n",
    "    # Create and save table\n",
    "    if results:\n",
    "        df = pd.DataFrame(results)\n",
    "        output_file = os.path.join(output_dir, f'cell_{cell_id}.csv')\n",
    "        df.to_csv(output_file, index=False)\n",
    "        return df\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "def process_all_cells(cell_ids, polygons_path, base_lc_path, base_veg_path, output_dir='celltables'):\n",
    "    \"\"\"Process all cells and save individual CSV files.\"\"\"\n",
    "    \n",
    "    # Load polygons\n",
    "    polygons_gdf = gpd.read_file(polygons_path)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Process each cell\n",
    "    for cell_id in cell_ids:\n",
    "        # Construct paths\n",
    "        landcover_nc= Path(base_lc_path) / f\"{cell_id}.nc\"\n",
    "        vegtype_nc = Path(base_veg_path) / f\"{cell_id}.nc\"\n",
    "        \n",
    "        process_cell(cell_id, polygons_gdf, landcover_nc, vegtype_nc, output_dir)\n",
    "                \n",
    "    \n",
    "   \n",
    "\n",
    "# Define paths (update these as needed)\n",
    "base_dir = '/home/muye/aus-env-modis/OCSE'\n",
    "output_dir = os.path.join(base_dir, 'celltables_by_division23')\n",
    "#polygons_path = '/home/muye/aus-env-modis/OCSE/actgov_districts.geojson'\n",
    "polygons_path = '/home/muye/aus-env-modis/OCSE/actgov_divsion_full.geojson'\n",
    "base_lc_path = '/home/muye/aus-env-modis/OCSE/lc23_byCells'\n",
    "base_veg_path = '/home/muye/aus-env-modis/OCSE/vegCode_byCells'\n",
    "\n",
    "cell_ids = np.arange(1, 42)\n",
    "process_all_cells(cell_ids, polygons_path, base_lc_path, base_veg_path, output_dir='celltables_by_division23')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### only a subset of cells (ACT scale) will intersect the divisions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing polygon BANKS\n",
      "\n",
      "Processing polygon CALWELL\n",
      "\n",
      "Processing polygon CONDER\n",
      "\n",
      "Processing polygon THEODORE\n",
      "\n",
      "Processing polygon AMAROO\n",
      "\n",
      "Processing polygon GUNGAHLIN\n",
      "\n",
      "Processing polygon NGUNNAWAL\n",
      "\n",
      "Processing polygon NICHOLLS\n",
      "\n",
      "Processing polygon TAYLOR\n",
      "\n",
      "Processing polygon MONCRIEFF\n",
      "\n",
      "Processing polygon CASEY\n",
      "\n",
      "Processing polygon FORDE\n",
      "\n",
      "Processing polygon BONNER\n",
      "\n",
      "Processing polygon JACKA\n",
      "\n",
      "Processing polygon CHARNWOOD\n",
      "\n",
      "Processing polygon DUNLOP\n",
      "\n",
      "Processing polygon FRASER\n",
      "\n",
      "Processing polygon HIGGINS\n",
      "\n",
      "Processing polygon HOLT\n",
      "\n",
      "Processing polygon LATHAM\n",
      "\n",
      "Processing polygon MACGREGOR\n",
      "\n",
      "Processing polygon STRATHNAIRN\n",
      "\n",
      "Processing polygon MACNAMARA\n",
      "\n",
      "Processing polygon HUME\n",
      "\n",
      "Processing polygon OAKS ESTATE\n",
      "\n",
      "Processing polygon PIALLIGO\n",
      "\n",
      "Processing polygon BEARD\n",
      "\n",
      "Processing polygon HAWKER\n",
      "\n",
      "Processing polygon SCULLIN\n",
      "\n",
      "Processing polygon WHITLAM\n",
      "\n",
      "Processing polygon HALL\n",
      "\n",
      "Processing polygon MITCHELL\n",
      "\n",
      "Processing polygon GIRALANG\n",
      "\n",
      "Processing polygon KALEEN\n",
      "\n",
      "Processing polygon LAWSON\n",
      "\n",
      "Processing polygon PALMERSTON\n",
      "\n",
      "Processing polygon CRACE\n",
      "\n",
      "Processing polygon FRANKLIN\n",
      "\n",
      "Processing polygon KAMBAH\n",
      "\n",
      "Processing polygon BARTON\n",
      "\n",
      "Processing polygon DEAKIN\n",
      "\n",
      "Processing polygon FORREST\n",
      "\n",
      "Processing polygon GRIFFITH\n",
      "\n",
      "Processing polygon KINGSTON\n",
      "\n",
      "Processing polygon NARRABUNDAH\n",
      "\n",
      "Processing polygon RED HILL\n",
      "\n",
      "Processing polygon YARRALUMLA\n",
      "\n",
      "Processing polygon SYMONSTON\n",
      "\n",
      "Processing polygon CURTIN\n",
      "\n",
      "Processing polygon GARRAN\n",
      "\n",
      "Processing polygon HUGHES\n",
      "\n",
      "Processing polygon ISAACS\n",
      "\n",
      "Processing polygon MAWSON\n",
      "\n",
      "Processing polygon O'MALLEY\n",
      "\n",
      "Processing polygon PEARCE\n",
      "\n",
      "Processing polygon PHILLIP\n",
      "\n",
      "Processing polygon AINSLIE\n",
      "\n",
      "Processing polygon DICKSON\n",
      "\n",
      "Processing polygon DOWNER\n",
      "\n",
      "Processing polygon HACKETT\n",
      "\n",
      "Processing polygon LYNEHAM\n",
      "\n",
      "Processing polygon WATSON\n",
      "\n",
      "Processing polygon KENNY\n",
      "\n",
      "Processing polygon CANBERRA AIRPORT\n",
      "\n",
      "Processing polygon FYSHWICK\n",
      "\n",
      "Processing polygon GORDON\n",
      "\n",
      "Processing polygon CHISHOLM\n",
      "\n",
      "Processing polygon FADDEN\n",
      "\n",
      "Processing polygon GILMORE\n",
      "\n",
      "Processing polygon GOWRIE\n",
      "\n",
      "Processing polygon ISABELLA PLAINS\n",
      "\n",
      "Processing polygon MACARTHUR\n",
      "\n",
      "Processing polygon MONASH\n",
      "\n",
      "Processing polygon RICHARDSON\n",
      "\n",
      "Processing polygon WANNIASSA\n",
      "\n",
      "Processing polygon THROSBY\n",
      "\n",
      "Processing polygon HARRISON\n",
      "\n",
      "Processing polygon COOMBS\n",
      "\n",
      "Processing polygon WRIGHT\n",
      "\n",
      "Processing polygon MOLONGLO\n",
      "\n",
      "Processing polygon DENMAN PROSPECT\n",
      "\n",
      "Processing polygon ARANDA\n",
      "\n",
      "Processing polygon BELCONNEN\n",
      "\n",
      "Processing polygon BRUCE\n",
      "\n",
      "Processing polygon COOK\n",
      "\n",
      "Processing polygon FLOREY\n",
      "\n",
      "Processing polygon MACQUARIE\n",
      "\n",
      "Processing polygon MCKELLAR\n",
      "\n",
      "Processing polygon PAGE\n",
      "\n",
      "Processing polygon WEETANGERA\n",
      "\n",
      "Processing polygon WESTON\n",
      "\n",
      "Processing polygon GREENWAY\n",
      "\n",
      "Processing polygon OXLEY\n",
      "\n",
      "Processing polygon CHAPMAN\n",
      "\n",
      "Processing polygon FISHER\n",
      "\n",
      "Processing polygon TORRENS\n",
      "\n",
      "Processing polygon CAMPBELL\n",
      "\n",
      "Processing polygon PARKES\n",
      "\n",
      "Processing polygon REID\n",
      "\n",
      "Processing polygon RUSSELL\n",
      "\n",
      "Processing polygon DUFFY\n",
      "\n",
      "Processing polygon HOLDER\n",
      "\n",
      "Processing polygon RIVETT\n",
      "\n",
      "Processing polygon STIRLING\n",
      "\n",
      "Processing polygon WARAMANGA\n",
      "\n",
      "Processing polygon CHIFLEY\n",
      "\n",
      "Processing polygon LYONS\n",
      "\n",
      "Processing polygon BONYTHON\n",
      "\n",
      "Processing polygon THARWA\n",
      "\n",
      "Processing polygon FARRER\n",
      "\n",
      "Processing polygon ACTON\n",
      "\n",
      "Processing polygon BRADDON\n",
      "\n",
      "Processing polygon O'CONNOR\n",
      "\n",
      "Processing polygon TURNER\n",
      "\n",
      "Processing polygon URIARRA VILLAGE\n",
      "\n",
      "Processing polygon EVATT\n",
      "\n",
      "Processing polygon FLYNN\n",
      "\n",
      "Processing polygon MELBA\n",
      "\n",
      "Processing polygon SPENCE\n",
      "\n",
      "Processing polygon CAPITAL HILL\n",
      "\n",
      "Processing polygon CITY\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def aggregate_polygon_data(cell_tables_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Aggregate cell-level data into polygon-level tables with weighted probabilities.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cell_tables_dir : str\n",
    "        Directory containing all cell-level tables\n",
    "    output_dir : str\n",
    "        Directory to save polygon-level aggregated tables\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Read all cell tables\n",
    "    cell_tables = []\n",
    "    for table_path in glob.glob(os.path.join(cell_tables_dir, \"*.csv\")):\n",
    "        df = pd.read_csv(table_path)\n",
    "        cell_tables.append(df)\n",
    "    \n",
    "    # Combine all cell tables\n",
    "    combined_df = pd.concat(cell_tables, ignore_index=True)\n",
    "    \n",
    "    # Get unique polygon (district) names\n",
    "    unique_polygons = combined_df['polygon'].unique()\n",
    "    \n",
    "    # Process each polygon\n",
    "    for polygon_name in unique_polygons:\n",
    "        print(f\"\\nProcessing polygon {polygon_name}\")\n",
    "        \n",
    "        # Filter data for current polygon\n",
    "        polygon_data = combined_df[combined_df['polygon'] == polygon_name]\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Group by vegetation type and land cover class\n",
    "        groups = polygon_data.groupby(['vegetation_type', 'landcover_class'])\n",
    "        \n",
    "        for (veg_type, lc_class), group in groups:\n",
    "            # Calculate weighted sum (numerator)\n",
    "            weighted_sum = np.sum(group['pixel_count'] * group['mean_probability'])\n",
    "            \n",
    "            # Calculate total pixel count (denominator)\n",
    "            total_pixels = np.sum(group['pixel_count'])\n",
    "            \n",
    "            # Calculate weighted probability\n",
    "            weighted_prob = weighted_sum / total_pixels if total_pixels > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'vegetation_type': veg_type,\n",
    "                'landcover_class': lc_class,\n",
    "                'total_pixels': total_pixels,\n",
    "                'weighted_probability': weighted_prob\n",
    "            })\n",
    "        \n",
    "        # Create and save polygon table\n",
    "        if results:\n",
    "            polygon_df = pd.DataFrame(results)\n",
    "            \n",
    "            # Sort for better readability\n",
    "            polygon_df = polygon_df.sort_values(['vegetation_type', 'landcover_class'])\n",
    "            \n",
    "            # Save table\n",
    "            output_file = os.path.join(output_dir, f'{polygon_name.replace(\" \", \"_\")}_aggregated.csv')\n",
    "            polygon_df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "# Define directories\n",
    "base_dir = '/home/muye/aus-env-modis/OCSE'\n",
    "cell_tables_dir = os.path.join(base_dir, 'celltables_by_division23')\n",
    "output_dir = os.path.join(base_dir, 'polygon_tables_division23')\n",
    "\n",
    "# Process the aggregation\n",
    "aggregate_polygon_data(cell_tables_dir, output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def combine_polygon_tables(polygon_tables_dir, output_file):\n",
    "    \"\"\"\n",
    "    Combine all polygon tables into a single CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    polygon_tables_dir : str\n",
    "        Directory containing all polygon-level tables\n",
    "    output_file : str\n",
    "        Path to the output combined CSV file\n",
    "    \"\"\"\n",
    "    # Get all polygon table files\n",
    "    table_files = glob.glob(os.path.join(polygon_tables_dir, '*_aggregated.csv'))\n",
    "    \n",
    "    # List to store all dataframes\n",
    "    dfs = []\n",
    "    \n",
    "    # Process each polygon table\n",
    "    for file_path in table_files:\n",
    "        # Read the table\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Extract polygon name from filename\n",
    "        # Remove '_aggregated.csv' and get the base name\n",
    "        polygon_name = Path(file_path).stem.replace('_aggregated', '')\n",
    "        \n",
    "        # Add polygon name as a column\n",
    "        df['polygon_name'] = polygon_name\n",
    "        \n",
    "        dfs.append(df)\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Reorder columns to put polygon_name first\n",
    "    columns_order = ['polygon_name', 'vegetation_type', 'landcover_class', \n",
    "                    'total_pixels', 'weighted_probability']\n",
    "    combined_df = combined_df[columns_order]\n",
    "    \n",
    "    # Sort by polygon_name, vegetation_type, and landcover_class\n",
    "    combined_df = combined_df.sort_values(['polygon_name', 'vegetation_type', 'landcover_class'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# Define directories and output file\n",
    "base_dir = '/home/muye/aus-env-modis/OCSE'\n",
    "polygon_tables_dir = os.path.join(base_dir, 'polygon_tables_division23')\n",
    "output_file = os.path.join(base_dir, 'lc23_byDivision.csv')\n",
    "\n",
    "# Combine tables\n",
    "combined_df = combine_polygon_tables(polygon_tables_dir, output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polygon_name</th>\n",
       "      <th>vegetation_type</th>\n",
       "      <th>landcover_class</th>\n",
       "      <th>total_pixels</th>\n",
       "      <th>weighted_probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACTON</td>\n",
       "      <td>381</td>\n",
       "      <td>0</td>\n",
       "      <td>1546</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACTON</td>\n",
       "      <td>381</td>\n",
       "      <td>1</td>\n",
       "      <td>1546</td>\n",
       "      <td>0.036025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACTON</td>\n",
       "      <td>381</td>\n",
       "      <td>2</td>\n",
       "      <td>1546</td>\n",
       "      <td>0.001011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACTON</td>\n",
       "      <td>381</td>\n",
       "      <td>3</td>\n",
       "      <td>1546</td>\n",
       "      <td>0.167204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACTON</td>\n",
       "      <td>381</td>\n",
       "      <td>4</td>\n",
       "      <td>1546</td>\n",
       "      <td>0.070390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  polygon_name  vegetation_type  landcover_class  total_pixels  \\\n",
       "0        ACTON              381                0          1546   \n",
       "1        ACTON              381                1          1546   \n",
       "2        ACTON              381                2          1546   \n",
       "3        ACTON              381                3          1546   \n",
       "4        ACTON              381                4          1546   \n",
       "\n",
       "   weighted_probability  \n",
       "0              0.000000  \n",
       "1              0.036025  \n",
       "2              0.001011  \n",
       "3              0.167204  \n",
       "4              0.070390  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/muye/aus-env-modis/OCSE/lc23_byDivision.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76709.23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['landcover_class'] == 0]['total_pixels'].sum() * 100 / 10000\n",
    "# ACT: 2358 sqkm2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549.36"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['polygon_name'] == 'WANNIASSA') & (df['landcover_class'] == 0)]['total_pixels'].sum() * 100 / 10000   #1.5km^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
